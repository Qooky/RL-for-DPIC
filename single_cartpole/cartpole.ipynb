{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstate space\\n0 |Cart Position | -4.8 4.8\\n1 |Cart Velocity |-Inf Inf \\n2 |Pole Angle | ~ -0.418 rad (-24째) ~ 0.418 rad (24째) \\n3 |Pole Angular Velocity |-Inf Inf\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "state space\n",
    "0 |Cart Position | -4.8 4.8\n",
    "1 |Cart Velocity |-Inf Inf \n",
    "2 |Pole Angle | ~ -0.418 rad (-24째) ~ 0.418 rad (24째) \n",
    "3 |Pole Angular Velocity |-Inf Inf\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mRecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "save simulation as mp4\n",
      "write text on a frame\n",
      "\u001b[0;31mInit docstring:\u001b[0m episode_num: the number of episode a file contains\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/cart-pole/RL-for-DPIC/cart-pole-env/lib/python3.8/site-packages/gym_recorder/recorder.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "Recorder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gym_recorder import Recorder\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self, render_mode=\"rgb_array\", n_observations=1000, q_table=np.array(None), alpha=0.5):\n",
    "      self.env = gym.make(\"CartPole-v1\", render_mode=render_mode)\n",
    "      self.action_space = self.env.action_space\n",
    "      self.observation_space = self.env.observation_space\n",
    "      self.n_observation_space = 4\n",
    "      self.n_buckets = 16\n",
    "      self.bucket_size = self.env.observation_space.high / (self.n_buckets/2)\n",
    "      self.q_table =  np.zeros((self.env.action_space.n, np.power(self.n_buckets, self.n_observation_space))) if not q_table.all() else q_table\n",
    "      self.epsilon = 1\n",
    "      self.n_episodes = n_observations\n",
    "      self.epsilon_delta = self.epsilon / self.n_episodes\n",
    "      self.metric = []\n",
    "      self.alpha = alpha\n",
    "      self.gamma =  0.99\n",
    "      return \n",
    "    \n",
    "    def get_index(self, observation):\n",
    "      observation_bucketed = self.bucket_observation(observation)\n",
    "      i = len(observation_bucketed) - 1\n",
    "      idx = 0\n",
    "      for el in observation_bucketed:\n",
    "        idx = idx + (self.n_buckets**i * el)\n",
    "        i -= 1\n",
    "      return int(idx)\n",
    "    \n",
    "    def bucket_observation(self, observation):\n",
    "      return(np.floor(observation/self.bucket_size))\n",
    "\n",
    "    def update_q_table(self, observation, observation_prime, reward, action=1):\n",
    "         \n",
    "      Q = self.q_table[action][self.get_index(observation)]\n",
    "      Q = (1-self.alpha)*Q + self.alpha*(reward + self.gamma*max(self.q_table[a][self.get_index(observation_prime)] for a in range(2))) \n",
    "      self.q_table[action][self.get_index(observation)] = Q\n",
    "\n",
    "    def train(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        j=0\n",
    "        for i in range(self.n_episodes):\n",
    "          action = self.policy(observation)\n",
    "          observation_prime, reward, terminated, truncated, info = self.env.step(action)\n",
    "          self.update_q_table(observation, observation_prime, reward, action)\n",
    "          j +=1\n",
    "          if terminated or truncated:\n",
    "            self.metric.append(j)\n",
    "            j=0\n",
    "            observation, info = self.env.reset()\n",
    "            # print(info)\n",
    "        self.env.close()\n",
    "      \n",
    "    def test(self):\n",
    "        # self.env.unwrapped.render_mode = \"human\"\n",
    "        vid = VideoRecorder(self.env, enabled=True, path=\"./qlearning_test_video/vid.mp4\")\n",
    "        observation, _ = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            vid.capture_frame()\n",
    "            index = self.get_index(observation)\n",
    "            if self.q_table[0][index] > self.q_table[1][index]:\n",
    "              action = 0\n",
    "            else:\n",
    "              action = 1 \n",
    "            observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "            if terminated or truncated:\n",
    "              # time.sleep(0.1)\n",
    "              observation, _ = self.env.reset()\n",
    "        self.env.reset()\n",
    "        self.env.close()\n",
    "        vid.close()\n",
    "            \n",
    "    def policy(self, observation):\n",
    "      self.epsilon = self.epsilon - np.log(self.epsilon_delta)   #at first, low probability to read from q-table, ie high prob take random action\n",
    "      take_random_action = self.epsilon < np.random.random()\n",
    "      if take_random_action:\n",
    "         return self.action_space.sample()\n",
    "      else:\n",
    "         index = self.get_index(observation)\n",
    "         if self.q_table[0][index] > self.q_table[1][index]:\n",
    "           return 0\n",
    "         else:\n",
    "           return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cart_Position=1.3460333\n",
      "Cart_Velocity=2.3611106e+38Pole_Angle=-0.18353856\n",
      "Pole_Angular_Velocity=-1.826864e+38\n"
     ]
    }
   ],
   "source": [
    "observation = env_alpha_75.observation_space.sample()\n",
    "# str([f\"{observe=}\" for observe in observation])\n",
    "nl = '\\n'\n",
    "Cart_Position = observation[0]\n",
    "Cart_Velocity = observation[1]\n",
    "Pole_Angle =observation[2]\n",
    "Pole_Angular_Velocity = observation[3]\n",
    "text = f\"{Cart_Position=}{nl}{Cart_Velocity=}{Pole_Angle=}{nl}{Pole_Angular_Velocity=}\"\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./qlearning_test_video/vid.mp4.\n",
      "Moviepy - Writing video ./qlearning_test_video/vid.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./qlearning_test_video/vid.mp4\n"
     ]
    }
   ],
   "source": [
    "env_alpha_75 = CartPoleEnv(n_observations=1000, alpha=0.75)\n",
    "env_alpha_75.train()\n",
    "env_alpha_75.test()\n",
    "plt.plot(env_alpha_75.metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gym_recorder import Recorder\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "class CartPoleEnvLinear(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self, render_mode=\"rgb_array\", n_observations=1000, q_table=np.array(None), alpha=0.5):\n",
    "      self.env = gym.make(\"CartPole-v1\", render_mode=render_mode)\n",
    "      self.action_space = self.env.action_space\n",
    "      self.observation_space = self.env.observation_space\n",
    "      self.n_observation_space = 4\n",
    "      self.n_buckets = 16\n",
    "      self.bucket_size = self.env.observation_space.high / (self.n_buckets/2)\n",
    "      self.q_table =  np.zeros((self.env.action_space.n, np.power(self.n_buckets, self.n_observation_space))) if not q_table.all() else q_table\n",
    "      self.epsilon = 1\n",
    "      self.n_episodes = n_observations\n",
    "      self.epsilon_delta = self.epsilon / self.n_episodes\n",
    "      self.metric = []\n",
    "      self.alpha = alpha\n",
    "      self.gamma =  0.99\n",
    "      return \n",
    "    \n",
    "    def get_index(self, observation):\n",
    "      observation_bucketed = self.bucket_observation(observation)\n",
    "      i = len(observation_bucketed) - 1\n",
    "      idx = 0\n",
    "      for el in observation_bucketed:\n",
    "        idx = idx + (self.n_buckets**i * el)\n",
    "        i -= 1\n",
    "      return int(idx)\n",
    "    \n",
    "    def bucket_observation(self, observation):\n",
    "      return(np.floor(observation/self.bucket_size))\n",
    "\n",
    "    def update_q_table(self, observation, observation_prime, reward, action=1):\n",
    "      Q = self.q_table[action][self.get_index(observation)]\n",
    "      Q = (1-self.alpha)*Q + self.alpha*(reward + self.gamma*max(self.q_table[a][self.get_index(observation_prime)] for a in range(2))) \n",
    "      self.q_table[action][self.get_index(observation)] = Q\n",
    "\n",
    "    def train(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        j=0\n",
    "        w = np.zeros(self.n_observation_space) + 1\n",
    "        for i in range(self.n_episodes):\n",
    "          action = self.policy(observation)\n",
    "          observation_prime, reward, terminated, truncated, info = self.env.step(action)\n",
    "          self.update_q_table(observation, observation_prime, reward, action)\n",
    "          j +=1\n",
    "          if terminated or truncated:\n",
    "            self.metric.append(j)\n",
    "            j=0\n",
    "            observation, info = self.env.reset()\n",
    "            # print(info)\n",
    "        self.env.close()\n",
    "      \n",
    "    def test(self):\n",
    "        # self.env.unwrapped.render_mode = \"human\"\n",
    "        vid = VideoRecorder(self.env, enabled=True, path=\"./qlearning_test_video/vid.mp4\")\n",
    "        observation, _ = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            vid.capture_frame()\n",
    "            index = self.get_index(observation)\n",
    "            if self.q_table[0][index] > self.q_table[1][index]:\n",
    "              action = 0\n",
    "            else:\n",
    "              action = 1 \n",
    "            observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "            if terminated or truncated:\n",
    "              # time.sleep(0.1)\n",
    "              observation, _ = self.env.reset()\n",
    "        self.env.reset()\n",
    "        self.env.close()\n",
    "        vid.close()\n",
    "            \n",
    "    def policy(self, observation):\n",
    "      self.epsilon = self.epsilon - np.log(self.epsilon_delta)   #at first, low probability to read from q-table, ie high prob take random action\n",
    "      take_random_action = self.epsilon < np.random.random()\n",
    "      if take_random_action:\n",
    "         return self.action_space.sample()\n",
    "      else:\n",
    "         index = self.get_index(observation)\n",
    "         if self.q_table[0][index] > self.q_table[1][index]:\n",
    "           return 0\n",
    "         else:\n",
    "           return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(env.n_observation_space) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
