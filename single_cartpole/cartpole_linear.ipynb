{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Features():\n",
    "    def __init__(self):\n",
    "        self.feature_matrix = []\n",
    "\n",
    "    def get_feature_matrix(self, n_actions, observation):\n",
    "        observation = self.get_engineered_feature(observation)\n",
    "        num_obs = len(observation)\n",
    "        features = []\n",
    "        for j in range(n_actions):\n",
    "          feature = [[None]*num_obs]*n_actions\n",
    "          feature[j] = observation\n",
    "          for i in range(j, n_actions-1+j):\n",
    "            feature[(i+1)%n_actions] = np.zeros(num_obs)\n",
    "          features.append(np.array(feature).flatten())\n",
    "        # self.feature_matrix = features\n",
    "        return features\n",
    "    \n",
    "    def get_engineered_feature(self, observation):\n",
    "      return observation\n",
    "\n",
    "# f = Features().get_feature_matrix(4, np.array([1,2,3,4]))\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "# class Features():\n",
    "#     def __init__(self):\n",
    "#         self.feature_matrix = []\n",
    "\n",
    "#     def get_feature_matrix(self, n_actions, observation):\n",
    "#         observation = self.get_engineered_feature(observation)\n",
    "#         num_obs = len(observation)\n",
    "#         features = []\n",
    "#         for j in range(n_actions):\n",
    "#           feature = [[None]*num_obs]*n_actions\n",
    "#           feature[j] = observation\n",
    "#           for i in range(j, n_actions-1+j):\n",
    "#             feature[(i+1)%n_actions] = np.zeros(num_obs)\n",
    "#           features.append(np.array(feature).flatten())\n",
    "#         self.feature_matrix = features\n",
    "    \n",
    "#     # def get_action(self, observation):\n",
    "#     #     features =self.get_feature_matrix(observation)\n",
    "#     #     action = np.argmax([feature for feature in features])\n",
    "#     #     delta = reward + self.gamma*max([feature for feature in features])\n",
    "#     #     self.w = (1-self.alpha)*(self.w) + self.alpha*(features[action]*delta) \n",
    "\n",
    "#     def get_engineered_feature(self, observation):\n",
    "#       return observation\n",
    "    \n",
    "class CartPoleEnvLinear(gym.Env):\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self, render_mode=\"rgb_array\", n_observations=1000, q_table=np.array(None), alpha=0.5):\n",
    "      self.env = gym.make(\"CartPole-v1\", render_mode=render_mode)\n",
    "      self.action_space = self.env.action_space\n",
    "      self.n_actions = 2\n",
    "      self.observation_space = self.env.observation_space\n",
    "      self.n_observation_space = 4\n",
    "      self.n_buckets = 4\n",
    "      self.bucket_size = self.env.observation_space.high / (self.n_buckets/2)\n",
    "      self.q_table =  np.zeros((self.env.action_space.n, np.power(self.n_buckets, self.n_observation_space))) if not q_table.all() else q_table\n",
    "      self.epsilon = 1\n",
    "      self.n_episodes = n_observations\n",
    "      self.epsilon_delta = self.epsilon / self.n_episodes\n",
    "      self.metric = []\n",
    "      self.test_metric = []\n",
    "      self.alpha = alpha\n",
    "      self.gamma =  0.99\n",
    "      self.w = np.zeros(self.n_observation_space*2)\n",
    "      self.features = Features()\n",
    "      return \n",
    "    \n",
    "    def get_index(self, observation):\n",
    "      observation_bucketed = self.bucket_observation(observation)\n",
    "      i = len(observation_bucketed) - 1\n",
    "      idx = 0\n",
    "      for el in observation_bucketed:\n",
    "        idx = idx + (self.n_buckets**i * el)\n",
    "        i -= 1\n",
    "      return int(idx)\n",
    "    \n",
    "    def bucket_observation(self, observation):\n",
    "      return(np.floor(observation/self.bucket_size))\n",
    "\n",
    "    def update_q_table(self, observation, observation_prime, reward, action=1):\n",
    "        action, features = self.get_action(observation=observation)\n",
    "        max_future_reward = self.get_max_future_reward(observation=observation_prime)\n",
    "        delta = reward + self.gamma*max_future_reward\n",
    "        self.w = (1-self.alpha)*(self.w) + self.alpha*(features[action]*delta)\n",
    "\n",
    "    def policy(self, observation):\n",
    "      self.epsilon = self.epsilon - self.epsilon_delta   #at first, low probability to read from q-table, ie high prob take random action\n",
    "      take_random_action = self.epsilon < np.random.random()\n",
    "      if take_random_action:\n",
    "        return self.action_space.sample()\n",
    "      else:\n",
    "        action, _ = self.get_action(observation)\n",
    "        return action\n",
    "      \n",
    "    def get_action(self, observation):\n",
    "       features = self.features.get_feature_matrix(n_actions=self.n_actions, observation=observation)\n",
    "       action = np.argmax([feature.dot(self.w) for feature in features])\n",
    "       return action, features\n",
    "       \n",
    "    def get_max_future_reward(self, observation):\n",
    "      features = self.features.get_feature_matrix(n_actions=self.n_actions, observation=observation)\n",
    "      max_reward = np.max([feature.dot(self.w) for feature in features])\n",
    "      return max_reward\n",
    "    \n",
    "    def train(self):\n",
    "        observation, _ = self.env.reset()\n",
    "        j=0\n",
    "        for i in range(self.n_episodes):\n",
    "          action = self.policy(observation)\n",
    "          observation_prime, reward, terminated, truncated, info = self.env.step(action)\n",
    "          self.update_q_table(observation, observation_prime, reward, action)\n",
    "          observation = observation_prime\n",
    "          j +=1\n",
    "          if terminated or truncated:\n",
    "            self.metric.append(j)\n",
    "            j=0\n",
    "            observation, info = self.env.reset()\n",
    "            # print(info)\n",
    "        self.env.close()\n",
    "      \n",
    "    def test(self):\n",
    "        vid = VideoRecorder(self.env, enabled=True, path=\"./qlearning_linear_test_video/vid.mp4\")\n",
    "        observation, _ = self.env.reset()\n",
    "        j = 0\n",
    "        for i in range(1000):\n",
    "            features_prime_0 = np.append(observation, np.zeros(len(observation))) # take action 0\n",
    "            features_prime_1 = np.append(np.zeros(len(observation)), observation) # take action 1\n",
    "            vid.capture_frame()\n",
    "            if features_prime_0.dot(self.w) > features_prime_1.dot(self.w):\n",
    "              action = 0\n",
    "            else:\n",
    "              action = 1 \n",
    "            observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "            j += 1\n",
    "            if terminated or truncated:\n",
    "              self.test_metric.append(j)\n",
    "              j = 0\n",
    "              observation, _ = self.env.reset()\n",
    "        self.env.reset()\n",
    "        self.env.close()\n",
    "        vid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = CartPoleEnvLinear(n_observations=100000)\n",
    "# env.train()\n",
    "# env.test()\n",
    "# plt.plot(env.metric)\n",
    "# plt.show()\n",
    "# plt.plot(env.test_metric)\n",
    "# plt.show\n",
    "# print(np.mean(env.metric))\n",
    "# print(np.median(env.metric))\n",
    "# print(np.quantile(env.metric, [0, 0.25, 0.5, 0.75, .95, .99, 0.999, 1]))\n",
    "# plt.hist(env.metric, bins=10, range=[50,np.max(env.metric)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=[]\n",
    "# r = 10000\n",
    "# seq = -np.arange(-r, r)\n",
    "# for x in seq:\n",
    "#     y.append(np.max((1/ (1+np.exp(-x*5/r)), 0.05)))\n",
    "# plt.plot(np.array(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cart-pole-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
